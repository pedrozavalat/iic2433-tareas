{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pontificia Universidad Católica de Chile <br>\n",
    "Departamento de Ciencia de la Computación <br>\n",
    "IIC2433 - Minería de Datos\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <h2> Tarea 4 </h2>\n",
    "    <h1> MLP y Regresión Logística </h1>\n",
    "    <p>\n",
    "        Profesor Marcelo Mendoza<br>\n",
    "        Segundo Semestre 2023<br>    \n",
    "        Fecha de entrega: 20 de Octubre\n",
    "    </p>\n",
    "    <br>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicaciones\n",
    "\n",
    "Deberás entregar **SOLO** el archivo .ipynb en el buzón respectivo en canvas.\n",
    "\n",
    "**IMPORTANTE**:\n",
    "- Se te dará puntaje tanto por código como por la manera en la que respondas las preguntas planteadas. Es decir, si tienes un código perfecto pero este no es explicado o no se responden preguntas asociadas a este, no se tendrá el puntaje completo.\n",
    "- El notebook debe tener todas las celdas de código ejecutadas. Cualquier notebook que no las tenga no podrá ser corregido.\n",
    "- El carácter de esta tarea es **INDIVIDUAL**. Cualquier instancia de copia resultará en un 1,1 como nota de curso.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se encuentran las librerías necesarias para elaborar la tarea. Recuerda ejecutar la celda antes de comenzar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import List\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto: Cáncer de mama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Octubre es dedicado como el \"Mes de la Sensibilización sobre el Cáncer de Mama\". Este tipo de cáncer afecta a millones de personas en el mundo, y a pesar de los avances médicos y campañas de sensibilización, sigue siendo una de las principales causas de muerte en mujeres.Uno de los desafíos más significativos es que, en muchos casos, el cáncer de mama no presenta síntomas evidentes en sus etapas iniciales. Esto significa que la detección temprana a través de exámenes regulares y la autoexploración mamaria son cruciales para mejorar las tasas de supervivencia y reducir la gravedad de la enfermedad en el momento del diagnóstico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En el ámbito de la minería de datos, las herramientas y técnicas que se enseñan en este curso adquieren un valor excepcional en el campo de la salud, especialmente en lo que respecta a la detección de enfermedades. La minería de datos ofrece una poderosa capacidad para analizar grandes conjuntos de información médica, identificar patrones y tendencias ocultas, y desarrollar modelos predictivos precisos. Por lo tanto, deseamos destacar la utilidad de los conocimientos y modelos que se adquieren en este curso, ya que tienen un impacto real y significativo en la ciencia médica.\n",
    "\n",
    "Gracias a la aplicación de la minería de datos, podemos mejorar sustancialmente la eficiencia de la detección de enfermedades. Esto significa que podemos identificar señales tempranas de enfermedades, realizar diagnósticos más precisos y predecir la progresión de condiciones médicas. En última instancia, esta capacidad tiene el potencial de salvar vidas al permitir intervenciones médicas más oportunas y efectivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (3 puntos) 🔥\n",
    "\n",
    "Al momento de escribir codigo es importante asegurarnos de que tanto nosotros como otras personas seran capaces de entenderlo. Con este fin, se utilizan diferentes medios como por ejemplo los comentarios al momento de implementar, docstrings para metodos, clases y modulos, type-hinting, entre otros. Debido a lo importante de esto, es que en esta tarea se otorgara un bonus de 3 puntos por el correcto uso de type-hinting al momento de declarar variables y funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Carga y Preprocesamiento de Datos (10 Pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta tarea, deberás utilizar el dataset [Breast Cancer Wisconsin](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data) que se puede encontrar en Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Carga de Datos (1 Pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos en la varible data\n",
    "data = pd.read_csv('breast_cancer_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Descripción del Dataset (3 Pts.)\n",
    "A continuación, presenta una descripción detallada del dataset. Se espera que investigues los datos, y expliques en que consisten al menos 9 columnas. Dentro de la explicación, menciona a qué tipo de datos corresponde cada columna. ¿Cuál es la columna objetivo? ¿Qué significa cada valor de esta columna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean   \n",
       "0    842302         M        17.99         10.38          122.80     1001.0  \\\n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean   \n",
       "0          0.11840           0.27760          0.3001              0.14710  \\\n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst   \n",
       "0  ...          17.33           184.60      2019.0            0.1622  \\\n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst   \n",
       "0             0.6656           0.7119                0.2654          0.4601  \\\n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(n=5) # Mostramos las primeras 5 columnas del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉🏻 El dataSet contiene las caracteristicas a partir de imagenes digitalizadas de una puncion aspirativa con aguja fina PAAF$^1$ de una masa mamaria. Estas caracteristicas describen al nucleo celular presente en la imagen. \n",
    "\n",
    "👉🏻 Cada fila del ```dataset``` corresponde a las caracteristicas de la imagen digitalizada de la masa mamaria. En ella podemos encontrar las siguientes caracteristicas: \n",
    "\n",
    "* **id**: Numero ID de la imagen digitalizada. Corresponde a una variable de numerica y el valor es de tipo entero. \n",
    "* **Diagnosis**: Carácter M o B, el cual hace referencia si el diagnosis del tejido mamario fue diagnosticada con un tumor benigno (las celulas no son cancerígenas), o maligno (las celulas son cancerígenas). Corresponde a una variable de tipo categorica, y el valor es de tipo texto.\n",
    "Las siguientes caracteristicas son variables de tipo continuas en donde el valor que presentan es de tipo decimal:\n",
    "* **radius**: Distancia promedio formada entre los puntos que se encuentran entre el perimetro hasta el centro del nucleo celular. \n",
    "* **perimeter_mean**: Tamaño promedio del tumor central de la imagen. \n",
    "* **area_mean**: Area promedio del tumor central de la imagen. \n",
    "* **radius_se**: Error estandar del promedio de las distancias desde el centro hasta los puntos del perimetro del tumor central de la imagen. \n",
    "* **radius_worst**: el valor promedio mas grande de los valores de las distancias promedio desde el punto central hasta los puntos del perimetro del tumor presente en la imagen. \n",
    "* **symmetry_mean**: Dentro del area especializada en la clasificacion de tumores cancerigenos, cuando nos referimos a la simetria de un tumor se basa en que tan regular o irregular es la forma del tumor. Dentro del contexto del dataset, el atributo \"Simestria promedio\" se refiere a la forma del tumor central de la imagen $^2$, es decir, cuantifica que tan irregular o simetrico es la forma del tumor en la imagen. \n",
    "* **area_se**: Error estandar del area promedio del tumor central de la imagen. \n",
    "\n",
    "👉🏻 Nuestra variable objetivo sera la caracteristica **diagnosis**, dado que nos permitira verificar el nivel de prediccion de nuestro modelo a implementar. Sera nuestra variable de respuesta ante los niveles de prediccion considerando las caracteristicas restantes del dataset. Como comentamos anteriormente, cada valor de la columna diagnosis corresponde a la clasificacion del tumor presentado en la imagen digitalizada. Este columna presenta dos valores: M y B, cuyo significado es si el tumor es maligno o benigno, respectivamente. \n",
    "\n",
    "\n",
    "$^{1\\text{ Método diagnóstico basado en la obtención de material citológico procedente de un nódulo o tumoración, para luego estudiarlo microscópicamente (Elsevier, 2008)}}$ [Link](https://www.elsevier.es/es-revista-anales-pediatria-continuada-51-articulo-puncion-aspirativa-con-aguja-fina-S1696281808748819)\n",
    "\n",
    "$^{2 \\text{ Clasificacion de la simetria o irregularidad de la forma de un tumor}}$ [Link](https://www.mdpi.com/2073-8994/15/3/571)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Limpieza del set de datos (3 Pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, deberás revisar el dataset y hacer una limpieza de los datos. Esto significa que deberás tratar posibles `datos nulos, outliers, columnas innecesarias`, etc. 🧐 Además, es importante que utilices One Hot Encoding para el diagnóstico, dejando 0 a las muestras benignas y 1 a las malignas.\n",
    "\n",
    "**IMPORTANTE:** más allá del código, lo más importante aquí es `explicar` lo que estás haciendo, las decisiones para limpiar que tomas y sobre todo `justificar`. Hay libertad en cuanto a lo que se puede hacer, pero es **importante** que se justifique cualquier procedimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En primer lugar eliminaremos la ultima columna \"Unnamed: 32\", la cual solo contiene valores nulos en el dataset\n",
    "data = data.drop(['Unnamed: 32'], axis=1)\n",
    "# Por otra parte, eliminaremos la columna ID de cada imagen, ya que para este caso no es una caracteristica que \n",
    "# describa numericamente las cualidades del tumor de la imagen. Solo la guardaremos por si es que la utilizamos en un futuro. \n",
    "id_column = data['id']\n",
    "data = data.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [diagnosis, radius_mean, texture_mean, perimeter_mean, area_mean, smoothness_mean, compactness_mean, concavity_mean, concave points_mean, symmetry_mean, fractal_dimension_mean, radius_se, texture_se, perimeter_se, area_se, smoothness_se, compactness_se, concavity_se, concave points_se, symmetry_se, fractal_dimension_se, radius_worst, texture_worst, perimeter_worst, area_worst, smoothness_worst, compactness_worst, concavity_worst, concave points_worst, symmetry_worst, fractal_dimension_worst]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Luego verificaremos si el dataset contiene datos nulos. En el caso que si, eliminamos las filas correspondientes. \n",
    "data[data.isnull().any(axis=1)]\n",
    "\n",
    "# Notamos que no existen filas nulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean   \n",
       "0            1        17.99         10.38          122.80     1001.0  \\\n",
       "1            1        20.57         17.77          132.90     1326.0   \n",
       "2            1        19.69         21.25          130.00     1203.0   \n",
       "3            1        11.42         20.38           77.58      386.1   \n",
       "4            1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564          1        21.56         22.39          142.00     1479.0   \n",
       "565          1        20.13         28.25          131.20     1261.0   \n",
       "566          1        16.60         28.08          108.30      858.1   \n",
       "567          1        20.60         29.33          140.10     1265.0   \n",
       "568          0         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean   \n",
       "0            0.11840           0.27760         0.30010              0.14710  \\\n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst   \n",
       "0           0.2419  ...        25.380          17.33           184.60  \\\n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst   \n",
       "0        2019.0           0.16220            0.66560           0.7119  \\\n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora cambiaremos el tipo atributo diagnosis a una variable de tipo cualitativa binaria, es decir, \n",
    "# utilizaremos one hot encodding para el diagnostico: B = 0 y M = 1\n",
    "data['diagnosis'] = data['diagnosis'].replace('B', 0)\n",
    "data['diagnosis'] = data['diagnosis'].replace('M', 1)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 En este caso implementamos deteccion de outliers usando LOF, ya que nos permite eliminar los outliers del dataset de manera directa. Solo consideramos los datos mas agrupados (inliners), mientras que los outliers son eliminados del dataset. Asi presentamos nos permite evitar la redundancia y presentar mejoras al momento de realizar predicciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finalmente eliminaremos los outliers del dataset detectandolos utilizando LOF para k = 5.\n",
    "\n",
    "# Primero normalizaremos el data mediante MinMax Scaler para verificar el umbral del corte\n",
    "data_scaled = MinMaxScaler().fit_transform(data.values)\n",
    "\n",
    "# Luego reducimos la dimensionalidad de data_normalized con un 90% de varianza retenida\n",
    "pca = PCA(0.90, whiten=True)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego de haber reducido la dimensionalidad a 6 componentes, obtenemos el umbral de corte \n",
    "# con 5 vecinos cercanos para cada dato\n",
    "lof = LocalOutlierFactor(n_neighbors=5, metric='euclidean')\n",
    "lof.fit_predict(data_pca)\n",
    "scores = lof.negative_outlier_factor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_hist(X: pd.DataFrame, title: str) -> None:\n",
    "    \"\"\" Funcion que genera un grafico de los valores de LOF del dataset \"\"\"\n",
    "    x, bins, patches = plt.hist(x=X, bins='auto', rwidth=0.85, color='darkred')\n",
    "    plt.grid(axis='y')\n",
    "    plt.xlabel('LOF')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    maxfreq = x.max()\n",
    "    plt.ylim(ymax=np.ceil(maxfreq))\n",
    "    \n",
    "plot_hist(scores, 'LOF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008787346221441126"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dado el grafico anterior, diremos que un umbral de corte sera aproximadamente -1.85\n",
    "def lof_threshold(v: float, scores: np.ndarray) -> float:\n",
    "    ''' Retorna la contaminacion del medio '''\n",
    "    N = len(scores)\n",
    "    points = scores[scores < v]\n",
    "    threshold = len(points)/N\n",
    "    return threshold \n",
    "\n",
    "lof_threshold(-1.85,scores) # Contaminacion del medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos que la contaminacion del medio es de un 0.0070298767 aproximadamente, y\n",
    "lof = LocalOutlierFactor(n_neighbors=5, metric='euclidean', contamination=0.00878734622)\n",
    "labels = lof.fit_predict(data_pca)\n",
    "\n",
    "# Obtenemos los indices de los datos que son inliners\n",
    "indices = list(np.where(labels==1)[0])\n",
    "\n",
    "# Eliminamos los outliers del dataset 'data', filtrando el dataset solo con los datos\n",
    "# que no son inliners (label = 1)\n",
    "data_without_outliers = data.values[indices,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 203, 212, 461, 491]\n"
     ]
    }
   ],
   "source": [
    "print(list(np.where(labels==-1)[0])) # Podemos ver que tenemos solamente 5 outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anterior: (569, 31), Actual: (564, 31)\n"
     ]
    }
   ],
   "source": [
    "# Ahora podemos visualizar el data anterior y el actual, tal que el actual no presenta los 5 outliers. \n",
    "print(f'Anterior: {data.shape}, Actual: {data_without_outliers.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        842302\n",
       "1        842517\n",
       "2      84300903\n",
       "4      84358402\n",
       "5        843786\n",
       "         ...   \n",
       "564      926424\n",
       "565      926682\n",
       "566      926954\n",
       "567      927241\n",
       "568       92751\n",
       "Name: id, Length: 564, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tambien, eliminaremos las imagenes que tambien corresponden a outliers\n",
    "id_column[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Normalización de datos (3 Pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normaliza los datos y responde las preguntas:\n",
    "1. ¿Por qué es necesario normalizar los datos?\n",
    "2. ¿Qué tipo de normalización se utilizó? ¿Por qué?\n",
    "3. ¿Qué columnas se normalizaron? ¿Por qué?\n",
    "4. Explique la diferencia entre el uso de `standard scaler` y `minmax scaler` como estrategia para normalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de normalizar, verificaremos el rango de valores para cada atributo e identificaremos\n",
    "# cuales son las caracteristicas que presentan rangos de valores fuera de [0,1]\n",
    "def get_columns_with_range_out_of_0_1(data: pd.DataFrame) -> List[str]:\n",
    "    ''' Retorna las columnas donde el rango de valores no esta en [0,1] '''\n",
    "    columns = []\n",
    "    for i in range(data.shape[1]):\n",
    "        if data[data.columns[i]].max() > 1 or data[data.columns[i]].min() > 1:\n",
    "            columns.append(i)\n",
    "    return columns\n",
    "\n",
    "# Obtenemos los indices de las caracteristicas que presentan valores fuera de [0,1]\n",
    "columns_index_with_range_out_0_1 = get_columns_with_range_out_of_0_1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos un escalamiento de los datos dentro de un intervalo [0,1]\n",
    "# Solo realizaremos un escalamiento de las columnas que no presentan datos que oscilan entre 0 y 1. \n",
    "scaler = MinMaxScaler()\n",
    "data_without_outliers[:, columns_index_with_range_out_0_1] = scaler.fit_transform(data_without_outliers[:, columns_index_with_range_out_0_1])\n",
    "\n",
    "# Redefinimos el dataset \n",
    "X_scaled = data_without_outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalizar los datos nos permite tener mejores resultados al momento de realizar predicciones. Ademas, evitamos la redundancia de datos y establecemos margen general de valores para todos los datos tal que no se presenta una gran dispersion entre ellos.\n",
    "\n",
    "2. Utilice el tipo de normalizacion ```MinMaxScaler```, ya que nos permite establecer todos los valores dentro de un mismo rango que varia entre 0 y 1, inclusive. Esto fue realizado, dado que nos permite asegurarnos que los datos estend entro de un intervalo fijo y no obtener grandes variaciones de valores entre ellos. Ademas, tener valores que oscilen entre 0 y 1 es mas comodo para modelos que presentan predicciones mediante estrategias probabilisticas. \n",
    "\n",
    "3. Normalice solo las columnas que presentan valores que no estan dentro del rango [0,1]. En este caso, son solos las columnas con indices en ```columns_index_with_range_out_0_1```. Esto nos permite mantener los valores originales de las caracteristicas ya con valores entre 0 y 1, y adecuar los valores que no estan dentro de la misma escala. \n",
    "\n",
    "4. *StandardScaler* es un tipo de normalizacion que transforma cada dato centrado entorno a la media, mientras que MinMaxScaler, o conocido como *Scaling*, establece cada dato dentro del intervalo [0,1]. Como estrategia para normalizar, StandardScaler funciona correctamente cuando los datos se distribuyen de manera normal. A diferencia de Scaling, esta preserva de mejor manera la distancia entre los puntos y es menos sensible frente a la presencia de outliers. Mientras que para MinMaxScaler, transforma cada dato en una probabilidad. No retiene de la mejor manera las distancias entre cada dato. Es mas sensible frente a la presencia de outliers. Y tambien, es mas preferible de usar cuando no se tiene conocimiento de la forma en que se distribuyen los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Partición training/testing (0 Pts.)\n",
    "Cree particiones de training/testing con test_size=0.3. Recuerda recuperar la variable objetivo \"y\" del dataset y `separarla` de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos la variable objetivo sin outliers y la eliminamos del dataset:\n",
    "y = X_scaled[:, 0] # Atributo 'diagnosis'\n",
    "\n",
    "# Eliminamos el atributo 'diagnosis' del dataset\n",
    "X_scaled = np.delete(X_scaled, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego creamos las particiones de training/testing con test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Perceptron y Multi Layer Perceptrón (25 Pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Perceptron (5 Pts.)\n",
    "Investigue sobre `Perceptron`🤖 y de una explicación de cómo funciona. `No es necesaria una explicación matemática`, el objetivo es que puedas tomar lo que aprendiste en clases o buscando en internet, y logres exponerlo de manera sintetizada para `demostrar tu aprendizaje` 🚀. Se espera que tu explicación contenga la respuesta a las siguientes preguntas: ¿Qué es? ¿Para qué sirve? ¿Cómo funciona? ¿Cuáles son sus ventajas y desventajas? ¿En qué situaciones se puede utilizar? ¿Qué tipo de problemas puede resolver? ¿Qué son los pesos y función de activación? ¿Cuáles son las limitaciones del modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi Layer Perceptron (5 Pts.)\n",
    "Ahora debes investigar sobre `Multi Layer Perceptron (MLP)` y nuevamente explicar con tus propias palabras cómo funciona. Debes poner énfasis en las principales diferencias y cambios que tiene con respecto al `Perceptron`. Agrega en tu desarrollo la respuesta a la siguiente pregunta: ¿Por qué es conveniente utilizar MLP para el dataset presentado en esta tarea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Crear y entrenar el modelo (7 Pts.)\n",
    "Entrene un clasificador de MLP con los datos de `entrenamiento`. Tienes libertad para modificar los hiperparámetros, cantidad de capas, neuronas, etc. Pero toda decisión debe ser `justificada`. Recuerda que el objetivo es obtener el mejor modelo posible. Para justificar tus decisiones puedes experimentar, buscar documentación o lo que estimes conveniente.\n",
    "\n",
    "**Importante:** No se evalúa que el modelo sea el mejor, si no que se justifiquen las decisiones tomadas. Es decir, no sirve de nada tener un porcentaje de acierto alto si no se justifica por qué se llegó a ese resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluar el modelo (2 Pts.)\n",
    "Evalúe el modelo con los datos de `testing` y calcule `accuracy`, `precision`, `recall` y `f1-score`. Puedes apoyarte de un reporte de clasificación. Comenta todos los resultados y explica qué significan 👀."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Matriz de confusión (6 Pts.)\n",
    "Genere una `matriz de confusión` normalizada del modelo y responda las preguntas:\n",
    "1. ¿Qué significa cada fila de la matriz?\n",
    "2. ¿Qué significa cada columna de la matriz?\n",
    "3. Explique error `tipo I` y error `tipo II` en base a la matriz de confusión.\n",
    "4. En relación al problema de cáncer de mama, ¿que tipo error es más grave? ¿Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Regresión Logística (20 Pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Teoría Regresión Logística (10 pts)\n",
    "Al igual que con Perceptron, investiga sobre `Regresión Lógistica` y da una explicación con tus propias palabras de cómo funciona. Nuevamente no se espera una demostración matemática, el objetivo es que demuestres tu aprendizaje. Puedes apoyarte de las siguientes preguntas guía: ¿Qué es? ¿Qué tipo de problemas resuelve? ¿Cómo se calcula la probabilidad? ¿Qué función de activación utiliza? ¿Qué se busca durante el proceso de entrenamiento? ¿Qué son los coeficientes? ¿Cómo se toma la decisión final de clasificación? ¿Cuál es la relación con la regresión lineal? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué es?**\n",
    "\n",
    "👉🏻 Regresion logistica es una estrategia estadistica que implementa un algoritmo de clasificacion binaria sobre un set de datos para predecir probabilisticamente la pertenencia de cada dato con respecto a alguna variable categorica. \n",
    "\n",
    "**¿Qué tipo de problemas resuelve?** \n",
    "\n",
    "👉🏻 Tiene como objetivo resolver problemas de clasificacion binaria. Ademas, Es una tecnica altamente recomendada para el analisis de datos, ya que se puede implementar con el fin de predecir eventos mediante probabilidades. Es usado para predecir variables categorias dependientes usando un conjunto de variables independientes. Por otra parte, tambien resuelve problemas de la evaluacion de modelos de prediccion, evaluando el rendimiento de este con las medidas _accuracy_, _precission_, _recall_ y _f1-score_.\n",
    "\n",
    "**¿Cómo se calcula la probabilidad?** \n",
    "\n",
    "👉🏻 Definamos $\\bold{x}$ y $\\bold{w}$ como el vector del conjunto de caracteristicas y el de los pesos de entradas (parametro del modelo), respectivamente, dentro de un espacio d-dimensional. \n",
    "$$\\bold{w} = \\begin{bmatrix}w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_d\\end{bmatrix}\\in\\mathbb{R}^{d+1} \\hspace{1cm} \\bold{x} = \\begin{bmatrix}1 \\\\ x_1 \\\\ \\vdots \\\\ x_d\\end{bmatrix} \\in {1} \\times \\mathbb{R}^d$$\n",
    "Para poder calcular la probabilidad de prediccion, Regresion logistica utiliza la funcion _sigmoid_ para transformar la sumatoria de las combinacion lineal $\\bold{w}^T\\bold{x}$ dentro de un dominio cerrado, tal que asi nos permite obtener la prediccion en terminos probabilisticos. En este sentido, podemos definir $s = \\bold{w}^T\\bold{x}$ y la funcion de _sigmoid_ como:\n",
    "$$\\theta(s) = \\frac{1}{1+e^{-s}}$$\n",
    "en donde $\\theta(s)$ solo se mueve entre 0 y 1.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190522162153/sigmoid-function-300x138.png\"></div>\n",
    "\n",
    "**¿Qué función de activación utiliza?** \n",
    "\n",
    "👉🏻 La funcion de activacion que utiliza la tecnica regresion logistica es la funcion logistica _sigmoid function_, en donde la suma de las combinaciones lineales entre cada caracteristica con un peso respectivo se transmiten a traves de esta funcion de activacion, y posteriormente esta determina un valor binario de 0 o 1, clasificando los datos. \n",
    "\n",
    "**¿Qué se busca durante el proceso de entrenamiento?** \n",
    "\n",
    "👉🏻 Durante el proceso de entrenamiento se busca el rendimiento del modelo a partir de la determinacion de los parametros del modelo, es decir, se tiene como objetivo buscar los parametros que \"maximizan\" la precision del modelo sobre la variable objetivo categorica, evaluando tanto su nivel de acierto y clasificacion. \n",
    "\n",
    "**¿Qué son los coeficientes?** \n",
    "\n",
    "👉🏻 Dentro del contexto del modelo de regresion logistica, determinamos la probabilidad de que una variable objetivo pertenezca a una categoria especifica (clasificacion binaria). Cada variable esta asociada a un coeficiente (o peso), en donde cada peso asignado puede tomar un valor positivo o negativo. Los coeficientes son los parametros estimados del modelo de regresion logistica. El valor de este parametro depende de la \"relevancia\" que presenta cada variable, por lo que se ajusta segun _la magnitud que  aporta_ la variable dentro de la prediccion del modelo. \n",
    "\n",
    "**¿Cómo se toma la decisión final de clasificación?** \n",
    "\n",
    "👉🏻 La decision final de clasificacion se determina en base al valor que toma la funcion de activacion. Esta clasificacion binaria se realiza de la siguiente manera. Por ejemplo, dado un set de imagenes, queremos clasificar las imagenes que contienen un arbol y las que no, es decir, si la probabilidad es mas cercana a 1 y sobre 0.5, entonces la imagen contiene un arbol. Mientras que si la probabilidad es mas cercana a 0 y menor o igual a 0.5, entonces la imagen no contiene un arbol. Esta logica de clasificacion binaria implementa el modelo de regresion logistica para decidir la asignación de cada dato con respecto a una variable categorica. En este sentido, si el valor retornado de la funcion es mas cercano a 1 (sobre 0.5), entonces el objeto a clasificar presenta una mayor probabilidad de ser clasificado correctamente (por ejemplo, como un arbol). En el caso contrario, si el valor retornado es mas cercano a 0, entonces el objeto no es clasificado como un arbol. \n",
    "\n",
    "**¿Cuál es la relación con la regresión lineal?** \n",
    "\n",
    "👉🏻 La relacion entre el modelo de regresion lineal y logistica recae en que ambos modelos determinan un valor de prediccion basado en variables independientes. La diferencia se presenta en que la regresion lineal es utilizado para predecir una variable continua, mientras que la regresion logistica es utilizado para predecir una variable categorica. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Crear y entrenar el modelo (3 Pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea y entrena un modelo de regresion logistica con los datos de entrenamiento, donde a traves del parametro `solver`, deberas elegir minimo 3 opciones diferentes de algoritmos de optimizacion, responder `cuales son sus principales diferencias` respecto a como actualizan los parametros del modelo y `dar una hipotesis` sobre cual crees que sera el algoritmo que funcione mejor para este dataset. Recuerda que la idea es que `justifiques tu respuesta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = ['liblinear', 'sag', 'newton-cg']\n",
    "\n",
    "clf_1 = LogisticRegression(solver='liblinear')\n",
    "clf_1.fit(X_train, y_train)\n",
    "\n",
    "clf_2 = LogisticRegression(solver='sag')\n",
    "clf_2.fit(X_train, y_train)\n",
    "\n",
    "clf_3 = LogisticRegression(solver='newton-cg')\n",
    "clf_3.fit(X_train, y_train)\n",
    "\n",
    "clfs = [clf_1, clf_2, clf_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluar el modelo (2 Pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para `cada uno` de los tres modelos anteriores, evalua el modelo con los datos de testing y calcula `accuracy`, `precision`, `recall` y `f1-score`. Puedes apoyarte de un reporte de clasificación. Comenta todos los resultados y explica qué significan 👀."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************ liblinear *************************\n",
      "                 Score: 0.9352941176470588                  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.99      0.95       103\n",
      "         1.0       0.98      0.85      0.91        67\n",
      "\n",
      "    accuracy                           0.94       170\n",
      "   macro avg       0.95      0.92      0.93       170\n",
      "weighted avg       0.94      0.94      0.93       170\n",
      "\n",
      "*************************** sag ****************************\n",
      "                 Score: 0.9470588235294117                  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.99      0.96       103\n",
      "         1.0       0.98      0.88      0.93        67\n",
      "\n",
      "    accuracy                           0.95       170\n",
      "   macro avg       0.96      0.94      0.94       170\n",
      "weighted avg       0.95      0.95      0.95       170\n",
      "\n",
      "************************ newton-cg *************************\n",
      "                 Score: 0.9470588235294117                  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.99      0.96       103\n",
      "         1.0       0.98      0.88      0.93        67\n",
      "\n",
      "    accuracy                           0.95       170\n",
      "   macro avg       0.96      0.94      0.94       170\n",
      "weighted avg       0.95      0.95      0.95       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, clf in enumerate(clfs):\n",
    "    print(f\" {solvers[index]} \".center(60, \"*\"))\n",
    "    print(f'Score: {clf.score(X_test, y_test)}'.center(60))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Matriz de confusión (5 Pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los modelos elegidos previamente, escoja el que mejor funcione y genere una `matriz de confusión` del modelo y responda la siguiente pregunta:\n",
    "\n",
    "1. ¿Que tan grave es el error que tenemos segun la matriz de confusión en el problema de cáncer de mama? ¿Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = clf_3.predict(X_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "classes = np.unique(y)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(5,5))\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Comparación de Modelos (5 Pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Comparación de modelos (5 Pts.)\n",
    "Una vez implementado cada modelo con los datos, compara los resultados obtenidos.\n",
    "1. ¿Que modelo posee el mejor `rendimiento` en este caso? ¿Por qué?\n",
    "2. ¿Qué métricas se utilizaron para comparar los modelos?\n",
    "3. ¿Qué ventajas y desventajas tiene cada modelo?\n",
    "4. ¿En que casos es mejor utilizar un modelo que otro? ¿Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
