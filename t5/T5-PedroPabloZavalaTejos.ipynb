{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3B6kdIC0A8j"
      },
      "source": [
        "Pontificia Universidad Católica de Chile <br>\n",
        "Departamento de Ciencia de la Computación <br>\n",
        "IIC2433 - Minería de Datos\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "    <h2> Tarea 5 </h2>\n",
        "    <h1> SVM </h1>\n",
        "    <p>\n",
        "        Profesor Marcelo Mendoza<br>\n",
        "        Segundo Semestre 2023<br> \n",
        "        Fecha de entrega: 3 de noviembre\n",
        "    </p>\n",
        "    <br>\n",
        "</center>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx4hXVuL2Lv-"
      },
      "source": [
        "## Indicaciones\n",
        "\n",
        "Deberás entregar **SOLO** el archivo .ipynb en el buzón respectivo en canvas.\n",
        "\n",
        "**IMPORTANTE**:\n",
        "- Se te dará puntaje tanto por código como por la manera en la que respondas las preguntas planteadas. Es decir, si tienes un código perfecto pero este no es explicado o no se responden preguntas asociadas a este, no se tendrá el puntaje completo.\n",
        "- El notebook debe tener todas las celdas de código ejecutadas. Cualquier notebook que no las tenga no podrá ser corregido.\n",
        "- El carácter de esta tarea es **INDIVIDUAL**. Cualquier instancia de copia resultará en un 1,1 como nota de curso.\n",
        "- En el caso de que se encuentren con problemas al correr celdas por el tamaño del dataset, esta permitido trabajar con una muestra representativa de este, siempre explicitando y justificando sus deciciones.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D20JLCp2NQy"
      },
      "source": [
        "## Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "jxFL6JoZ2k9D"
      },
      "outputs": [],
      "source": [
        "##Importa acá las librerias que vayas a utilizar\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "\n",
        "# from lime.lime_text import LimeTextExplainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Contexto:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se tiene un dataset con información de distintas noticias, entre las cuales hay noticias falsas y verdaderas. El objetivo de esta tarea es predecir si una noticia es falsa o verdadera, utilizando distintos modelos de clasificación.. Para esto, primero se debe hacer un análisis exploratorio de los datos para decidir qué variables son relevantes para el problema. Luego, se debe vectorizar el texto de loas noticias para poder utilizarlo en un modelo de clasificación (en esta tarea usaremos SBert y TF-IDF). Después se debe entrenar un modelo de clasificación SVM que prediga si una noticia es falsa o verdadera. Finalmente, se debe evaluar el modelo y compararlo con otros modelos de clasificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Objetivos de la tarea:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Realizar un análisis exploratorio de datos para determinar las variables pertinentes para el problema en cuestión, identificando características clave y patrones en los datos que podrían afectar la clasificación de las noticias.\n",
        "- Dominar la técnica de vectorización de texto para facilitar su aplicación en un modelo de clasificación, y comprender la interpretación de los resultados obtenidos.\n",
        "- Entender el funcionamiento de un modelo de clasificación SVM y sus hiperparámetros.\n",
        "- Entrenar un modelo de clasificación SVM capaz de predecir si una noticia es falsa o verdadera.\n",
        "- Evaluar el desempeño del modelo y analizar en profundidad los resultados obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAwvXdDO36uO"
      },
      "source": [
        "# Parte 1: Carga y Preprocesamiento (10 puntos)\n",
        "\n",
        "## 1.1 Carga de datos (1 punto)\n",
        "\n",
        "Para esta tarea deberás trabajar con el dataset que está en Canvas, 'news.csv'. Este dataset contiene información de distintas noticias, entre las cuales hay noticias falsas y verdaderas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>authenticity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>U.S. Agriculture secretary nominee submits eth...</td>\n",
              "      <td>(Reuters) - U.S. President Donald Trump’s nomi...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 13, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>Trump aides attack agency that will analyze he...</td>\n",
              "      <td>WASHINGTON (Reuters) - Aides to U.S. President...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 12, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>Highlights: The Trump presidency on March 12 a...</td>\n",
              "      <td>(Reuters) - Highlights of the day for U.S. Pre...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 12, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>Obama lawyers move fast to join fight against ...</td>\n",
              "      <td>WASHINGTON (Reuters) - When Johnathan Smith re...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 13, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>Mike Pence to tour Asia next month amid securi...</td>\n",
              "      <td>JAKARTA (Reuters) - U.S. Vice President Mike P...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 13, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title  \\\n",
              "0      Donald Trump Sends Out Embarrassing New Year’...   \n",
              "1      Drunk Bragging Trump Staffer Started Russian ...   \n",
              "2      Sheriff David Clarke Becomes An Internet Joke...   \n",
              "3      Trump Is So Obsessed He Even Has Obama’s Name...   \n",
              "4      Pope Francis Just Called Out Donald Trump Dur...   \n",
              "...                                                 ...   \n",
              "9995  U.S. Agriculture secretary nominee submits eth...   \n",
              "9996  Trump aides attack agency that will analyze he...   \n",
              "9997  Highlights: The Trump presidency on March 12 a...   \n",
              "9998  Obama lawyers move fast to join fight against ...   \n",
              "9999  Mike Pence to tour Asia next month amid securi...   \n",
              "\n",
              "                                                   text       subject  \\\n",
              "0     Donald Trump just couldn t wish all Americans ...          News   \n",
              "1     House Intelligence Committee Chairman Devin Nu...          News   \n",
              "2     On Friday, it was revealed that former Milwauk...          News   \n",
              "3     On Christmas day, Donald Trump announced that ...          News   \n",
              "4     Pope Francis used his annual Christmas Day mes...          News   \n",
              "...                                                 ...           ...   \n",
              "9995  (Reuters) - U.S. President Donald Trump’s nomi...  politicsNews   \n",
              "9996  WASHINGTON (Reuters) - Aides to U.S. President...  politicsNews   \n",
              "9997  (Reuters) - Highlights of the day for U.S. Pre...  politicsNews   \n",
              "9998  WASHINGTON (Reuters) - When Johnathan Smith re...  politicsNews   \n",
              "9999  JAKARTA (Reuters) - U.S. Vice President Mike P...  politicsNews   \n",
              "\n",
              "                   date authenticity  \n",
              "0     December 31, 2017         Fake  \n",
              "1     December 31, 2017         Fake  \n",
              "2     December 30, 2017         Fake  \n",
              "3     December 29, 2017         Fake  \n",
              "4     December 25, 2017         Fake  \n",
              "...                 ...          ...  \n",
              "9995    March 13, 2017          Real  \n",
              "9996    March 12, 2017          Real  \n",
              "9997    March 12, 2017          Real  \n",
              "9998    March 13, 2017          Real  \n",
              "9999    March 13, 2017          Real  \n",
              "\n",
              "[10000 rows x 5 columns]"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('news.csv', sep=',')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['News', 'politicsNews'], dtype=object)"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['subject'].unique() # Verificamos cuantas categorias presenta el data set en terminos del tipo de noticia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjqBcwhUGa2j"
      },
      "source": [
        "## 1.2 Descripcion del Dataset (5 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptkeso4_NZbp"
      },
      "source": [
        "\n",
        "\n",
        "¿Qué representa cada feature en el dataset entregado? Refiérete a su tipo de dato, detallando cómo trabajar con los datos no númericos (2 puntos)\n",
        "\n",
        "RESPUESTA:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "👉🏻 A partir de los atributos visualiazdos en el set de datos, podemos verificar que todas las variables son de tipo texto, excepto date que es de tipo fecha. \n",
        "* ```title```: Representa el titulo de la noticia. Es una variable categorica nominal, ya que no representa un orden propuesto dentro de la tabulacion.  \n",
        "* ```texto```: Representa el contexto de la noticia. Es una variable categorica nominal. \n",
        "Como title y texts no son variables numericas, para poder trabajarlas realizaremos un preprocesamiento el cual consistira en eliminar las palabras comunes \n",
        "y luego utilizar algun lematizador para poder reducir las cadenas de oraciones a su forma base, es decir, reducirlas a su raiz semántica, eliminando \n",
        "prefijos y sufijos. Esto nos permitira reducir la complejidad en cada tupla y analizar comparativamente con el resto del set de datos .\n",
        "* ```date```: Representa la fecha en la cual la noticia fue publicada. Como no es un dato numerico, podemos transformarlo a de tipo numerico para su posterior analisis utilizando \n",
        "el valor en representacion \"Unix Time\". Unix time convierte la fecha actual en la cantidad de segundos desde el primer dia del año. En este sentido, trabajaremos con fechas en segundos \n",
        "para poder compararlos con los otros datos y mejorar analisis. \n",
        "* ```Subject```: Representa el tema relatado en la noticia. En este caso corresponde a una variable categorica nominal. Podemos visualizar que solo contiene dos categorias: News y PolitcNews. En el caso de news representa a un reporte general de un suceso, mientras que PoliticNews esta determinado dentro del contexto de la politica. Por lo tanto, podemos transformar esta caracteristica a una variable numerica binaria: 0 si es News y 1 si es politicNews.  \n",
        "* ```Authenticity```: Representa que tan veridico es la noticia. Es una variable categorica nominal. Presenta dos resultados (Fake/Real). Por lo tanto, podemos trabajar esta variable como una variable binaria. Donde 0 es para el caso de que sea Fake, y 1 si es Real. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JSFOZ7lRzA5"
      },
      "source": [
        "¿Qué columnas o features crees son relevantes para el problema? ¿Por qué? (3 puntos)\n",
        "\n",
        "RESPUESTA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "👉🏻 Por un lado, la variable de respuesta para poder predecir si la noticia es verdadera o falsa sera la columna __authenticity__. Ya que, nos permitiria verificar el rendimiento de nuestro modelo con respecto a los valores reales. Por otro lado, tanto el titulo como el texto nos permitiran verificar a que clase corresponde la noticia, ya que al tener las palabras claves de la noticia nos permitirá clasificar segun el tema que esta referencia. Nos permitirá comparar los resultados entre si y obtener un mejor grado de prediccion. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeCYvsCyRXUU"
      },
      "source": [
        "## 1.3 Datos nulos (2 puntos)\n",
        "\n",
        "Analiza la presencia de valores nulos en el conjunto de datos y cómo se distribuyen en las distintas columnas. Después, toma una decisión acerca del tratamiento adecuado para el dataframe con respecto a los valores nulos. Justifica tu decisión.\n",
        "\n",
        "RESPUESTA:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "title           0\n",
              "text            0\n",
              "subject         0\n",
              "date            0\n",
              "authenticity    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Al verificar la presencia de datos nulos, podemos visualizar que no existen \n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhm6ZjBnRix7"
      },
      "source": [
        "## 1.4 Manejo del Dataset (2 puntos)\n",
        "\n",
        "- Elimina las columnas que no sean relevantes para el entrenamiento del modelo (1 pts.)\n",
        "- Haz que los textos estén en un formato óptimo para ser procesados por los modelos de clasificación (0.5 pts.) Justifica tu decisión (0.5 pts.)\n",
        "\n",
        "\n",
        "\n",
        "RESPUESTA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# En primer lugar, eliminaremos la columna the date, ya que consideraremos que no es relevante para la prediccion de la vericidad de las noticias\n",
        "data.drop(columns=['date'], inplace=True)\n",
        "# Asimismo, eliminaremos la columna  subject, ya que consideraremos que no es un aporte para la prediccion del modelo. \n",
        "# Es decir, consideraremos solo title y text como caracteristicas esenciales para verificar si la noticia es verdadera o falsa. \n",
        "data.drop(columns=['subject'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Luego transformaremos la caracteristicas authenticity a tipo binaria y la guardaremos como nuestra variable objetivo Y\n",
        "# Fake = 0, Real = 1\n",
        "data['authenticity'] = data['authenticity'].replace('Fake', 0)\n",
        "data['authenticity'] = data['authenticity'].replace('Real', 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = data['authenticity'] # Variable de respuesta binaria\n",
        "data.drop(columns=['authenticity'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finalmente uniremos tanto el titulo y el texto de la noticia en una sola caracteristica: \"fullContent\". \n",
        "data['fullContent'] = data['title'] + ' ' + data['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dejaremos el dataset solo con el contenido completo entre titulo y texto\n",
        "data.drop(columns=['title', 'text'], inplace=True) # Eliminamos la caracteristica title y text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Finalmente, defininiremos la funcion tokenize para que conserve las palabras elementales de cada fila (reducir cada palabra a su raiz semantica). \n",
        "stop_words = set(stopwords.words('english')) # Eliminamos stop words como and, the, etc ... \n",
        "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')  \n",
        "lemmatizer = WordNetLemmatizer() # Inicializamos el Lematizador\n",
        "\n",
        "def tokenize(document):\n",
        "    words = []\n",
        "    cant_words = 0\n",
        "    for sentence in sent_tokenize(document):\n",
        "        # Check if the sentence contains a URL\n",
        "        if re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', sentence):\n",
        "            continue  # Skip this sentence or handle it as you wish\n",
        "        tokens = [lemmatizer.lemmatize(t.lower()) \n",
        "                for t in tokenizer.tokenize(sentence) \n",
        "                if t.lower() not in stop_words \n",
        "                and len(t) > 2]\n",
        "        words += tokens\n",
        "    text = ' '.join(words)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['fullContent'] = data['fullContent'].apply(tokenize) # Aplicamos tokenize para la cadena de texto completa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "👉🏻 Tener la columna FullContent reducida de tal manera nos permitira preprocesar mejor los datos, luego de ser vectorizados, asi podemos predecir con mayor acierto en la autenticidad de la noticia. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 2: Vectorización (14 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SBert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para esta parte, deben utilizar SBert para vectorizar los textos de las noticias. Utilicen el modelo pre-entrenado de SBert llamado SentenceTransformer. Específicamente, deben usar el modelo 'paraphrase-MiniLM-L6-v2' para obtener las representaciones vectoriales de las oraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1) Vectorización con SBert (3 puntos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "2479",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 2479",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCatólicadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mparaphrase-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Generate embeddings for the train, validation, and test sets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(train_df[\u001b[39m'\u001b[39;49m\u001b[39mfullContent\u001b[39;49m\u001b[39m'\u001b[39;49m], show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# val_embeddings = model.encode(val_df['fullContent'], show_progress_bar=True, batch_size=16)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# test_embeddings = model.encode(test_df['fullContent'], show_progress_bar=True, batch_size=16)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# embeddings = model.encode(data['fullContent'], show_progress_bar=True, batch_size=16)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m   1042\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 2479"
          ]
        }
      ],
      "source": [
        "# TODO \n",
        "# Split the data into train, validation, and test sets\n",
        "train_df, temp_df = train_test_split(data, stratify=Y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# Initialize the transformer model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for the train, validation, and test sets\n",
        "train_embeddings = model.encode(train_df['fullContent'], show_progress_bar=True, batch_size=16)\n",
        "# val_embeddings = model.encode(val_df['fullContent'], show_progress_bar=True, batch_size=16)\n",
        "# test_embeddings = model.encode(test_df['fullContent'], show_progress_bar=True, batch_size=16)\n",
        "\n",
        "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "# embeddings = model.encode(data['fullContent'], show_progress_bar=True, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Análisis teórico de SBert (4 puntos)\n",
        "Responde las siguientes preguntas: ¿Qué es SBert? ¿Cómo funciona? ¿Qué ventajas y desventajas tiene sobre otros modelos de vectorización de texto?\n",
        "\n",
        "RESPUESTA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__¿Qué es SBert?__ \n",
        "\n",
        "👉🏻 \n",
        "\n",
        "__¿Cómo funciona?__  \n",
        "\n",
        "👉🏻 \n",
        "\n",
        "__¿Qué ventajas y desventajas tiene sobre otros modelos de vectorización de texto?__\n",
        "\n",
        "👉🏻"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para esta parte, deben utilizar TF-IDF para vectorizar los textos de las noticias. Utilicen la clase TfidfVectorizer de la librería sklearn.\n",
        "\n",
        "\n",
        "Tip: limita la cantidad de features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3) Vectorización con TF-IDF (3 puntos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_TFIDF = vectorizer.fit_transform(data['fullContent'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>abortion</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>access</th>\n",
              "      <th>according</th>\n",
              "      <th>account</th>\n",
              "      <th>accusation</th>\n",
              "      <th>accused</th>\n",
              "      <th>across</th>\n",
              "      <th>...</th>\n",
              "      <th>world</th>\n",
              "      <th>worse</th>\n",
              "      <th>would</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>year</th>\n",
              "      <th>yes</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.038686</td>\n",
              "      <td>0.046997</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.442866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.131510</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073722</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155821</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.022584</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100945</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040547</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.062268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.066983</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.099532</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.097605</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.149132</td>\n",
              "      <td>0.036234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.048243</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.058135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.185910</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034941</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034896</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039124</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021459</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ability  able  abortion  absolutely  access  according  account  \\\n",
              "0         0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "1         0.0   0.0       0.0         0.0     0.0   0.131510      0.0   \n",
              "2         0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "3         0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "4         0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "...       ...   ...       ...         ...     ...        ...      ...   \n",
              "9995      0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "9996      0.0   0.0       0.0         0.0     0.0   0.024251      0.0   \n",
              "9997      0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "9998      0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "9999      0.0   0.0       0.0         0.0     0.0   0.034896      0.0   \n",
              "\n",
              "      accusation  accused  across  ...     world  worse     would     wrong  \\\n",
              "0            0.0      0.0     0.0  ...  0.000000    0.0  0.038686  0.046997   \n",
              "1            0.0      0.0     0.0  ...  0.073722    0.0  0.000000  0.000000   \n",
              "2            0.0      0.0     0.0  ...  0.000000    0.0  0.000000  0.000000   \n",
              "3            0.0      0.0     0.0  ...  0.000000    0.0  0.100945  0.000000   \n",
              "4            0.0      0.0     0.0  ...  0.062268    0.0  0.000000  0.000000   \n",
              "...          ...      ...     ...  ...       ...    ...       ...       ...   \n",
              "9995         0.0      0.0     0.0  ...  0.000000    0.0  0.099532  0.000000   \n",
              "9996         0.0      0.0     0.0  ...  0.000000    0.0  0.149132  0.036234   \n",
              "9997         0.0      0.0     0.0  ...  0.000000    0.0  0.048243  0.000000   \n",
              "9998         0.0      0.0     0.0  ...  0.000000    0.0  0.034941  0.000000   \n",
              "9999         0.0      0.0     0.0  ...  0.039124    0.0  0.021459  0.000000   \n",
              "\n",
              "      wrote      year  yes       yet      york     young  \n",
              "0       0.0  0.442866  0.0  0.000000  0.000000  0.000000  \n",
              "1       0.0  0.000000  0.0  0.000000  0.155821  0.000000  \n",
              "2       0.0  0.022584  0.0  0.000000  0.000000  0.000000  \n",
              "3       0.0  0.040547  0.0  0.000000  0.000000  0.000000  \n",
              "4       0.0  0.000000  0.0  0.066983  0.000000  0.094625  \n",
              "...     ...       ...  ...       ...       ...       ...  \n",
              "9995    0.0  0.000000  0.0  0.097605  0.000000  0.000000  \n",
              "9996    0.0  0.035942  0.0  0.000000  0.000000  0.000000  \n",
              "9997    0.0  0.058135  0.0  0.000000  0.185910  0.000000  \n",
              "9998    0.0  0.000000  0.0  0.000000  0.000000  0.000000  \n",
              "9999    0.0  0.025859  0.0  0.000000  0.000000  0.000000  \n",
              "\n",
              "[10000 rows x 1000 columns]"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_TFIDF = pd.DataFrame(X_TFIDF.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "df_TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2) Análisis teórico de TF-IDF (4 puntos)\n",
        "Responde las siguientes preguntas: ¿Qué es TF-IDF? ¿Cómo funciona? ¿Qué ventajas y desventajas tiene sobre otros modelos de vectorización de texto?\n",
        "\n",
        "RESPUESTA:\n",
        "\n",
        "__¿Qué es TF-IDF?__\n",
        "\n",
        "👉🏻\n",
        "\n",
        "__¿Cómo funciona?__\n",
        "\n",
        "👉🏻\n",
        "\n",
        "__¿Qué ventajas y desventajas tiene sobre otros modelos de vectorización de texto?__\n",
        "\n",
        "👉🏻\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVspTAuVGsUM"
      },
      "source": [
        "# Parte 3: SVM (36 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1) Preguntas teóricas (10 puntos)\n",
        "1. ¿Qué es SVM? ¿Cómo funciona? ¿En qué casos es útil? ¿En cuáles no? (4 ptos.)\n",
        "3. ¿Qué es un kernel? ¿Qué tipos de kernels existen? ¿Cuál es la diferencia entre ellos? (3 ptos.)\n",
        "4. ¿Qué indica el parámetro C? ¿Qué sucede si C es muy grande? ¿Y si es muy pequeño? (3 ptos.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2) SVM con vectorización SBert (10 puntos) \n",
        "Para esta parte deben entrenar un modelo SVM con las representaciones vectoriales obtenidas con SBert. Deben utilizar la clase SVC de la librería sklearn. Para esto:\n",
        "- Deben dejar los datos en un formato adecuado para ser procesados por el modelo (1 pto.)\n",
        "- Dividir el dataset en train y test (1 pto.)\n",
        "- Entrenar el modelo SVM con los datos de train (2 ptos.) modificando los hiperparámetros kernel y C (4 ptos.)\n",
        "- Evaluar el modelo con los datos de test y comentar brevemente los resultados obtenidos (2 ptos.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3) SVM con vectorización TF-IDF (10 puntos) \n",
        "Para esta parte deben entrenar un modelo SVM con las representaciones vectoriales obtenidas con TF-IDF. Deben utilizar la clase SVC de la librería sklearn. Para esto:\n",
        "- Deben dejar los datos en un formato adecuado para ser procesados por el modelo (1 pto.)\n",
        "- Dividir el dataset en train y test (1 pto.)\n",
        "- Entrenar el modelo SVM con los datos de train (2 ptos.) modificando los hiperparámetros kernel y C (4 ptos.)\n",
        "- Evaluar el modelo con los datos de test y comentar brevemente los resultados obtenidos (2 ptos.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "MinMaxScaler does not support sparse input. Consider using MaxAbsScaler instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCatólicadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb Cell 45\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_TF_IDF_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(X_TFIDF)\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sklearn/base.py:852\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:416\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y)\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    442\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinimum of desired feature range must be smaller than maximum. Got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(feature_range)\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m     )\n\u001b[1;32m    452\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    453\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    454\u001b[0m     X,\n\u001b[1;32m    455\u001b[0m     reset\u001b[39m=\u001b[39mfirst_pass,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m     force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    459\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: MinMaxScaler does not support sparse input. Consider using MaxAbsScaler instead."
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4) Análisis de resultados (6 puntos)\n",
        "- ¿Qué vectorización obtuvo mejores resultados, SBert o TF-IDF? ¿Por qué? (3 ptos.)\n",
        "- ¿Qué hiperparámetros obtuvieron mejores resultados para cada vectorización? (3 ptos.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 4 (Bonus): LIME explainer (5 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuación haz un análisis de los resultados obtenidos con el modelo SVM con vectorización SBert utilizando LIME explainer. Para esto, debes seguir los siguientes pasos:\n",
        "- Instalar la librería Lime\n",
        "- Elegir un ejemplo de test\n",
        "- Utilizar el explainer de Lime para explicar la predicción del modelo en ese ejemplo (2.5 pts.)\n",
        "- Analizar los resultados obtenidos y comentar brevemente (2.5 pts.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
