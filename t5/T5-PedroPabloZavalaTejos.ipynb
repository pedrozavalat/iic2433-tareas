{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3B6kdIC0A8j"
      },
      "source": [
        "Pontificia Universidad Cat√≥lica de Chile <br>\n",
        "Departamento de Ciencia de la Computaci√≥n <br>\n",
        "IIC2433 - Miner√≠a de Datos\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "    <h2> Tarea 5 </h2>\n",
        "    <h1> SVM </h1>\n",
        "    <p>\n",
        "        Profesor Marcelo Mendoza<br>\n",
        "        Segundo Semestre 2023<br> \n",
        "        Fecha de entrega: 3 de noviembre\n",
        "    </p>\n",
        "    <br>\n",
        "</center>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx4hXVuL2Lv-"
      },
      "source": [
        "## Indicaciones\n",
        "\n",
        "Deber√°s entregar **SOLO** el archivo .ipynb en el buz√≥n respectivo en canvas.\n",
        "\n",
        "**IMPORTANTE**:\n",
        "- Se te dar√° puntaje tanto por c√≥digo como por la manera en la que respondas las preguntas planteadas. Es decir, si tienes un c√≥digo perfecto pero este no es explicado o no se responden preguntas asociadas a este, no se tendr√° el puntaje completo.\n",
        "- El notebook debe tener todas las celdas de c√≥digo ejecutadas. Cualquier notebook que no las tenga no podr√° ser corregido.\n",
        "- El car√°cter de esta tarea es **INDIVIDUAL**. Cualquier instancia de copia resultar√° en un 1,1 como nota de curso.\n",
        "- En el caso de que se encuentren con problemas al correr celdas por el tama√±o del dataset, esta permitido trabajar con una muestra representativa de este, siempre explicitando y justificando sus deciciones.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D20JLCp2NQy"
      },
      "source": [
        "## Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "jxFL6JoZ2k9D"
      },
      "outputs": [],
      "source": [
        "##Importa ac√° las librerias que vayas a utilizar\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "\n",
        "# from lime.lime_text import LimeTextExplainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Contexto:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se tiene un dataset con informaci√≥n de distintas noticias, entre las cuales hay noticias falsas y verdaderas. El objetivo de esta tarea es predecir si una noticia es falsa o verdadera, utilizando distintos modelos de clasificaci√≥n.. Para esto, primero se debe hacer un an√°lisis exploratorio de los datos para decidir qu√© variables son relevantes para el problema. Luego, se debe vectorizar el texto de loas noticias para poder utilizarlo en un modelo de clasificaci√≥n (en esta tarea usaremos SBert y TF-IDF). Despu√©s se debe entrenar un modelo de clasificaci√≥n SVM que prediga si una noticia es falsa o verdadera. Finalmente, se debe evaluar el modelo y compararlo con otros modelos de clasificaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Objetivos de la tarea:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Realizar un an√°lisis exploratorio de datos para determinar las variables pertinentes para el problema en cuesti√≥n, identificando caracter√≠sticas clave y patrones en los datos que podr√≠an afectar la clasificaci√≥n de las noticias.\n",
        "- Dominar la t√©cnica de vectorizaci√≥n de texto para facilitar su aplicaci√≥n en un modelo de clasificaci√≥n, y comprender la interpretaci√≥n de los resultados obtenidos.\n",
        "- Entender el funcionamiento de un modelo de clasificaci√≥n SVM y sus hiperpar√°metros.\n",
        "- Entrenar un modelo de clasificaci√≥n SVM capaz de predecir si una noticia es falsa o verdadera.\n",
        "- Evaluar el desempe√±o del modelo y analizar en profundidad los resultados obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAwvXdDO36uO"
      },
      "source": [
        "# Parte 1: Carga y Preprocesamiento (10 puntos)\n",
        "\n",
        "## 1.1 Carga de datos (1 punto)\n",
        "\n",
        "Para esta tarea deber√°s trabajar con el dataset que est√° en Canvas, 'news.csv'. Este dataset contiene informaci√≥n de distintas noticias, entre las cuales hay noticias falsas y verdaderas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>authenticity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year‚Äô...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama‚Äôs Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>Fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>U.S. Agriculture secretary nominee submits eth...</td>\n",
              "      <td>(Reuters) - U.S. President Donald Trump‚Äôs nomi...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 13, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>Trump aides attack agency that will analyze he...</td>\n",
              "      <td>WASHINGTON (Reuters) - Aides to U.S. President...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 12, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>Highlights: The Trump presidency on March 12 a...</td>\n",
              "      <td>(Reuters) - Highlights of the day for U.S. Pre...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 12, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>Obama lawyers move fast to join fight against ...</td>\n",
              "      <td>WASHINGTON (Reuters) - When Johnathan Smith re...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 13, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>Mike Pence to tour Asia next month amid securi...</td>\n",
              "      <td>JAKARTA (Reuters) - U.S. Vice President Mike P...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>March 13, 2017</td>\n",
              "      <td>Real</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows √ó 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title  \\\n",
              "0      Donald Trump Sends Out Embarrassing New Year‚Äô...   \n",
              "1      Drunk Bragging Trump Staffer Started Russian ...   \n",
              "2      Sheriff David Clarke Becomes An Internet Joke...   \n",
              "3      Trump Is So Obsessed He Even Has Obama‚Äôs Name...   \n",
              "4      Pope Francis Just Called Out Donald Trump Dur...   \n",
              "...                                                 ...   \n",
              "9995  U.S. Agriculture secretary nominee submits eth...   \n",
              "9996  Trump aides attack agency that will analyze he...   \n",
              "9997  Highlights: The Trump presidency on March 12 a...   \n",
              "9998  Obama lawyers move fast to join fight against ...   \n",
              "9999  Mike Pence to tour Asia next month amid securi...   \n",
              "\n",
              "                                                   text       subject  \\\n",
              "0     Donald Trump just couldn t wish all Americans ...          News   \n",
              "1     House Intelligence Committee Chairman Devin Nu...          News   \n",
              "2     On Friday, it was revealed that former Milwauk...          News   \n",
              "3     On Christmas day, Donald Trump announced that ...          News   \n",
              "4     Pope Francis used his annual Christmas Day mes...          News   \n",
              "...                                                 ...           ...   \n",
              "9995  (Reuters) - U.S. President Donald Trump‚Äôs nomi...  politicsNews   \n",
              "9996  WASHINGTON (Reuters) - Aides to U.S. President...  politicsNews   \n",
              "9997  (Reuters) - Highlights of the day for U.S. Pre...  politicsNews   \n",
              "9998  WASHINGTON (Reuters) - When Johnathan Smith re...  politicsNews   \n",
              "9999  JAKARTA (Reuters) - U.S. Vice President Mike P...  politicsNews   \n",
              "\n",
              "                   date authenticity  \n",
              "0     December 31, 2017         Fake  \n",
              "1     December 31, 2017         Fake  \n",
              "2     December 30, 2017         Fake  \n",
              "3     December 29, 2017         Fake  \n",
              "4     December 25, 2017         Fake  \n",
              "...                 ...          ...  \n",
              "9995    March 13, 2017          Real  \n",
              "9996    March 12, 2017          Real  \n",
              "9997    March 12, 2017          Real  \n",
              "9998    March 13, 2017          Real  \n",
              "9999    March 13, 2017          Real  \n",
              "\n",
              "[10000 rows x 5 columns]"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('news.csv', sep=',')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['News', 'politicsNews'], dtype=object)"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['subject'].unique() # Verificamos cuantas categorias presenta el data set en terminos del tipo de noticia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjqBcwhUGa2j"
      },
      "source": [
        "## 1.2 Descripcion del Dataset (5 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptkeso4_NZbp"
      },
      "source": [
        "\n",
        "\n",
        "¬øQu√© representa cada feature en el dataset entregado? Refi√©rete a su tipo de dato, detallando c√≥mo trabajar con los datos no n√∫mericos (2 puntos)\n",
        "\n",
        "RESPUESTA:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üëâüèª A partir de los atributos visualiazdos en el set de datos, podemos verificar que todas las variables son de tipo texto, excepto date que es de tipo fecha. \n",
        "* ```title```: Representa el titulo de la noticia. Es una variable categorica nominal, ya que no representa un orden propuesto dentro de la tabulacion.  \n",
        "* ```texto```: Representa el contexto de la noticia. Es una variable categorica nominal. \n",
        "Como title y texts no son variables numericas, para poder trabajarlas realizaremos un preprocesamiento el cual consistira en eliminar las palabras comunes \n",
        "y luego utilizar algun lematizador para poder reducir las cadenas de oraciones a su forma base, es decir, reducirlas a su raiz sem√°ntica, eliminando \n",
        "prefijos y sufijos. Esto nos permitira reducir la complejidad en cada tupla y analizar comparativamente con el resto del set de datos .\n",
        "* ```date```: Representa la fecha en la cual la noticia fue publicada. Como no es un dato numerico, podemos transformarlo a de tipo numerico para su posterior analisis utilizando \n",
        "el valor en representacion \"Unix Time\". Unix time convierte la fecha actual en la cantidad de segundos desde el primer dia del a√±o. En este sentido, trabajaremos con fechas en segundos \n",
        "para poder compararlos con los otros datos y mejorar analisis. \n",
        "* ```Subject```: Representa el tema relatado en la noticia. En este caso corresponde a una variable categorica nominal. Podemos visualizar que solo contiene dos categorias: News y PolitcNews. En el caso de news representa a un reporte general de un suceso, mientras que PoliticNews esta determinado dentro del contexto de la politica. Por lo tanto, podemos transformar esta caracteristica a una variable numerica binaria: 0 si es News y 1 si es politicNews.  \n",
        "* ```Authenticity```: Representa que tan veridico es la noticia. Es una variable categorica nominal. Presenta dos resultados (Fake/Real). Por lo tanto, podemos trabajar esta variable como una variable binaria. Donde 0 es para el caso de que sea Fake, y 1 si es Real. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JSFOZ7lRzA5"
      },
      "source": [
        "¬øQu√© columnas o features crees son relevantes para el problema? ¬øPor qu√©? (3 puntos)\n",
        "\n",
        "RESPUESTA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üëâüèª Por un lado, la variable de respuesta para poder predecir si la noticia es verdadera o falsa sera la columna __authenticity__. Ya que, nos permitiria verificar el rendimiento de nuestro modelo con respecto a los valores reales. Por otro lado, tanto el titulo como el texto nos permitiran verificar a que clase corresponde la noticia, ya que al tener las palabras claves de la noticia nos permitir√° clasificar segun el tema que esta referencia. Nos permitir√° comparar los resultados entre si y obtener un mejor grado de prediccion. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeCYvsCyRXUU"
      },
      "source": [
        "## 1.3 Datos nulos (2 puntos)\n",
        "\n",
        "Analiza la presencia de valores nulos en el conjunto de datos y c√≥mo se distribuyen en las distintas columnas. Despu√©s, toma una decisi√≥n acerca del tratamiento adecuado para el dataframe con respecto a los valores nulos. Justifica tu decisi√≥n.\n",
        "\n",
        "RESPUESTA:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "title           0\n",
              "text            0\n",
              "subject         0\n",
              "date            0\n",
              "authenticity    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Al verificar la presencia de datos nulos, podemos visualizar que no existen \n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhm6ZjBnRix7"
      },
      "source": [
        "## 1.4 Manejo del Dataset (2 puntos)\n",
        "\n",
        "- Elimina las columnas que no sean relevantes para el entrenamiento del modelo (1 pts.)\n",
        "- Haz que los textos est√©n en un formato √≥ptimo para ser procesados por los modelos de clasificaci√≥n (0.5 pts.) Justifica tu decisi√≥n (0.5 pts.)\n",
        "\n",
        "\n",
        "\n",
        "RESPUESTA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# En primer lugar, eliminaremos la columna the date, ya que consideraremos que no es relevante para la prediccion de la vericidad de las noticias\n",
        "data.drop(columns=['date'], inplace=True)\n",
        "# Asimismo, eliminaremos la columna  subject, ya que consideraremos que no es un aporte para la prediccion del modelo. \n",
        "# Es decir, consideraremos solo title y text como caracteristicas esenciales para verificar si la noticia es verdadera o falsa. \n",
        "data.drop(columns=['subject'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Luego transformaremos la caracteristicas authenticity a tipo binaria y la guardaremos como nuestra variable objetivo Y\n",
        "# Fake = 0, Real = 1\n",
        "data['authenticity'] = data['authenticity'].replace('Fake', 0)\n",
        "data['authenticity'] = data['authenticity'].replace('Real', 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = data['authenticity'] # Variable de respuesta binaria\n",
        "data.drop(columns=['authenticity'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finalmente uniremos tanto el titulo y el texto de la noticia en una sola caracteristica: \"fullContent\". \n",
        "data['fullContent'] = data['title'] + ' ' + data['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dejaremos el dataset solo con el contenido completo entre titulo y texto\n",
        "data.drop(columns=['title', 'text'], inplace=True) # Eliminamos la caracteristica title y text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Finalmente, defininiremos la funcion tokenize para que conserve las palabras elementales de cada fila (reducir cada palabra a su raiz semantica). \n",
        "stop_words = set(stopwords.words('english')) # Eliminamos stop words como and, the, etc ... \n",
        "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')  \n",
        "lemmatizer = WordNetLemmatizer() # Inicializamos el Lematizador\n",
        "\n",
        "def tokenize(document):\n",
        "    words = []\n",
        "    cant_words = 0\n",
        "    for sentence in sent_tokenize(document):\n",
        "        # Check if the sentence contains a URL\n",
        "        if re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', sentence):\n",
        "            continue  # Skip this sentence or handle it as you wish\n",
        "        tokens = [lemmatizer.lemmatize(t.lower()) \n",
        "                for t in tokenizer.tokenize(sentence) \n",
        "                if t.lower() not in stop_words \n",
        "                and len(t) > 2]\n",
        "        words += tokens\n",
        "    text = ' '.join(words)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['fullContent'] = data['fullContent'].apply(tokenize) # Aplicamos tokenize para la cadena de texto completa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üëâüèª Tener la columna FullContent reducida de tal manera nos permitira preprocesar mejor los datos, luego de ser vectorizados, asi podemos predecir con mayor acierto en la autenticidad de la noticia. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 2: Vectorizaci√≥n (14 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SBert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para esta parte, deben utilizar SBert para vectorizar los textos de las noticias. Utilicen el modelo pre-entrenado de SBert llamado SentenceTransformer. Espec√≠ficamente, deben usar el modelo 'paraphrase-MiniLM-L6-v2' para obtener las representaciones vectoriales de las oraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1) Vectorizaci√≥n con SBert (3 puntos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "2479",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 2479",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCatoÃÅlicadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39mparaphrase-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Generate embeddings for the train, validation, and test sets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(train_df[\u001b[39m'\u001b[39;49m\u001b[39mfullContent\u001b[39;49m\u001b[39m'\u001b[39;49m], show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# val_embeddings = model.encode(val_df['fullContent'], show_progress_bar=True, batch_size=16)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# test_embeddings = model.encode(test_df['fullContent'], show_progress_bar=True, batch_size=16)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#Y102sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# embeddings = model.encode(data['fullContent'], show_progress_bar=True, batch_size=16)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m   1042\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 2479"
          ]
        }
      ],
      "source": [
        "# TODO \n",
        "# Split the data into train, validation, and test sets\n",
        "train_df, temp_df = train_test_split(data, stratify=Y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# Initialize the transformer model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for the train, validation, and test sets\n",
        "train_embeddings = model.encode(train_df['fullContent'], show_progress_bar=True, batch_size=16)\n",
        "# val_embeddings = model.encode(val_df['fullContent'], show_progress_bar=True, batch_size=16)\n",
        "# test_embeddings = model.encode(test_df['fullContent'], show_progress_bar=True, batch_size=16)\n",
        "\n",
        "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "# embeddings = model.encode(data['fullContent'], show_progress_bar=True, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 An√°lisis te√≥rico de SBert (4 puntos)\n",
        "Responde las siguientes preguntas: ¬øQu√© es SBert? ¬øC√≥mo funciona? ¬øQu√© ventajas y desventajas tiene sobre otros modelos de vectorizaci√≥n de texto?\n",
        "\n",
        "RESPUESTA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__¬øQu√© es SBert?__ \n",
        "\n",
        "üëâüèª \n",
        "\n",
        "__¬øC√≥mo funciona?__  \n",
        "\n",
        "üëâüèª \n",
        "\n",
        "__¬øQu√© ventajas y desventajas tiene sobre otros modelos de vectorizaci√≥n de texto?__\n",
        "\n",
        "üëâüèª"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para esta parte, deben utilizar TF-IDF para vectorizar los textos de las noticias. Utilicen la clase TfidfVectorizer de la librer√≠a sklearn.\n",
        "\n",
        "\n",
        "Tip: limita la cantidad de features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3) Vectorizaci√≥n con TF-IDF (3 puntos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_TFIDF = vectorizer.fit_transform(data['fullContent'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>abortion</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>access</th>\n",
              "      <th>according</th>\n",
              "      <th>account</th>\n",
              "      <th>accusation</th>\n",
              "      <th>accused</th>\n",
              "      <th>across</th>\n",
              "      <th>...</th>\n",
              "      <th>world</th>\n",
              "      <th>worse</th>\n",
              "      <th>would</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>year</th>\n",
              "      <th>yes</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.038686</td>\n",
              "      <td>0.046997</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.442866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.131510</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073722</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155821</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.022584</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100945</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040547</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.062268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.066983</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.099532</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.097605</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.149132</td>\n",
              "      <td>0.036234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.048243</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.058135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.185910</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034941</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034896</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039124</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021459</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows √ó 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ability  able  abortion  absolutely  access  according  account  \\\n",
              "0         0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "1         0.0   0.0       0.0         0.0     0.0   0.131510      0.0   \n",
              "2         0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "3         0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "4         0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "...       ...   ...       ...         ...     ...        ...      ...   \n",
              "9995      0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "9996      0.0   0.0       0.0         0.0     0.0   0.024251      0.0   \n",
              "9997      0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "9998      0.0   0.0       0.0         0.0     0.0   0.000000      0.0   \n",
              "9999      0.0   0.0       0.0         0.0     0.0   0.034896      0.0   \n",
              "\n",
              "      accusation  accused  across  ...     world  worse     would     wrong  \\\n",
              "0            0.0      0.0     0.0  ...  0.000000    0.0  0.038686  0.046997   \n",
              "1            0.0      0.0     0.0  ...  0.073722    0.0  0.000000  0.000000   \n",
              "2            0.0      0.0     0.0  ...  0.000000    0.0  0.000000  0.000000   \n",
              "3            0.0      0.0     0.0  ...  0.000000    0.0  0.100945  0.000000   \n",
              "4            0.0      0.0     0.0  ...  0.062268    0.0  0.000000  0.000000   \n",
              "...          ...      ...     ...  ...       ...    ...       ...       ...   \n",
              "9995         0.0      0.0     0.0  ...  0.000000    0.0  0.099532  0.000000   \n",
              "9996         0.0      0.0     0.0  ...  0.000000    0.0  0.149132  0.036234   \n",
              "9997         0.0      0.0     0.0  ...  0.000000    0.0  0.048243  0.000000   \n",
              "9998         0.0      0.0     0.0  ...  0.000000    0.0  0.034941  0.000000   \n",
              "9999         0.0      0.0     0.0  ...  0.039124    0.0  0.021459  0.000000   \n",
              "\n",
              "      wrote      year  yes       yet      york     young  \n",
              "0       0.0  0.442866  0.0  0.000000  0.000000  0.000000  \n",
              "1       0.0  0.000000  0.0  0.000000  0.155821  0.000000  \n",
              "2       0.0  0.022584  0.0  0.000000  0.000000  0.000000  \n",
              "3       0.0  0.040547  0.0  0.000000  0.000000  0.000000  \n",
              "4       0.0  0.000000  0.0  0.066983  0.000000  0.094625  \n",
              "...     ...       ...  ...       ...       ...       ...  \n",
              "9995    0.0  0.000000  0.0  0.097605  0.000000  0.000000  \n",
              "9996    0.0  0.035942  0.0  0.000000  0.000000  0.000000  \n",
              "9997    0.0  0.058135  0.0  0.000000  0.185910  0.000000  \n",
              "9998    0.0  0.000000  0.0  0.000000  0.000000  0.000000  \n",
              "9999    0.0  0.025859  0.0  0.000000  0.000000  0.000000  \n",
              "\n",
              "[10000 rows x 1000 columns]"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_TFIDF = pd.DataFrame(X_TFIDF.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "df_TFIDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2) An√°lisis te√≥rico de TF-IDF (4 puntos)\n",
        "Responde las siguientes preguntas: ¬øQu√© es TF-IDF? ¬øC√≥mo funciona? ¬øQu√© ventajas y desventajas tiene sobre otros modelos de vectorizaci√≥n de texto?\n",
        "\n",
        "RESPUESTA:\n",
        "\n",
        "__¬øQu√© es TF-IDF?__\n",
        "\n",
        "üëâüèª\n",
        "\n",
        "__¬øC√≥mo funciona?__\n",
        "\n",
        "üëâüèª\n",
        "\n",
        "__¬øQu√© ventajas y desventajas tiene sobre otros modelos de vectorizaci√≥n de texto?__\n",
        "\n",
        "üëâüèª\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVspTAuVGsUM"
      },
      "source": [
        "# Parte 3: SVM (36 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1) Preguntas te√≥ricas (10 puntos)\n",
        "1. ¬øQu√© es SVM? ¬øC√≥mo funciona? ¬øEn qu√© casos es √∫til? ¬øEn cu√°les no? (4 ptos.)\n",
        "3. ¬øQu√© es un kernel? ¬øQu√© tipos de kernels existen? ¬øCu√°l es la diferencia entre ellos? (3 ptos.)\n",
        "4. ¬øQu√© indica el par√°metro C? ¬øQu√© sucede si C es muy grande? ¬øY si es muy peque√±o? (3 ptos.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2) SVM con vectorizaci√≥n SBert (10 puntos) \n",
        "Para esta parte deben entrenar un modelo SVM con las representaciones vectoriales obtenidas con SBert. Deben utilizar la clase SVC de la librer√≠a sklearn. Para esto:\n",
        "- Deben dejar los datos en un formato adecuado para ser procesados por el modelo (1 pto.)\n",
        "- Dividir el dataset en train y test (1 pto.)\n",
        "- Entrenar el modelo SVM con los datos de train (2 ptos.) modificando los hiperpar√°metros kernel y C (4 ptos.)\n",
        "- Evaluar el modelo con los datos de test y comentar brevemente los resultados obtenidos (2 ptos.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3) SVM con vectorizaci√≥n TF-IDF (10 puntos) \n",
        "Para esta parte deben entrenar un modelo SVM con las representaciones vectoriales obtenidas con TF-IDF. Deben utilizar la clase SVC de la librer√≠a sklearn. Para esto:\n",
        "- Deben dejar los datos en un formato adecuado para ser procesados por el modelo (1 pto.)\n",
        "- Dividir el dataset en train y test (1 pto.)\n",
        "- Entrenar el modelo SVM con los datos de train (2 ptos.) modificando los hiperpar√°metros kernel y C (4 ptos.)\n",
        "- Evaluar el modelo con los datos de test y comentar brevemente los resultados obtenidos (2 ptos.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "MinMaxScaler does not support sparse input. Consider using MaxAbsScaler instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCatoÃÅlicadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb Cell 45\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pedro/Library/CloudStorage/OneDrive-UniversidadCato%CC%81licadeChile/Onedrive_UC/sexto-semestre/mineria-de-datos/iic2433-tareas-pedro-zavala/t5/T5-PedroPabloZavalaTejos.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_TF_IDF_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(X_TFIDF)\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sklearn/base.py:852\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:416\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y)\n",
            "File \u001b[0;32m~/miniforge3/envs/cv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    442\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinimum of desired feature range must be smaller than maximum. Got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(feature_range)\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m     )\n\u001b[1;32m    452\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    453\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    454\u001b[0m     X,\n\u001b[1;32m    455\u001b[0m     reset\u001b[39m=\u001b[39mfirst_pass,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m     force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    459\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: MinMaxScaler does not support sparse input. Consider using MaxAbsScaler instead."
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4) An√°lisis de resultados (6 puntos)\n",
        "- ¬øQu√© vectorizaci√≥n obtuvo mejores resultados, SBert o TF-IDF? ¬øPor qu√©? (3 ptos.)\n",
        "- ¬øQu√© hiperpar√°metros obtuvieron mejores resultados para cada vectorizaci√≥n? (3 ptos.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 4 (Bonus): LIME explainer (5 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuaci√≥n haz un an√°lisis de los resultados obtenidos con el modelo SVM con vectorizaci√≥n SBert utilizando LIME explainer. Para esto, debes seguir los siguientes pasos:\n",
        "- Instalar la librer√≠a Lime\n",
        "- Elegir un ejemplo de test\n",
        "- Utilizar el explainer de Lime para explicar la predicci√≥n del modelo en ese ejemplo (2.5 pts.)\n",
        "- Analizar los resultados obtenidos y comentar brevemente (2.5 pts.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
